{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drpetros11111/Deep-Learning-and-Neural-Networks-Theory-and-Applications-with-PyTorch/blob/main/CNN_MNIST_Classify_your_own_images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdsCAMwTgoXU"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import the Libraries\n",
        " These lines are importing necessary libraries for your Python script, likely for a deep learning task, possibly involving image processing. Here's a breakdown:\n",
        "\n",
        "-------------------\n",
        "##import cv2:\n",
        "This imports the OpenCV (cv2) library.\n",
        "\n",
        "OpenCV is a popular library for computer vision tasks. You'll likely use it for reading, manipulating, and processing images.\n",
        "\n",
        "##import torch:\n",
        "This imports the PyTorch library, which is a fundamental library for deep learning.\n",
        "\n",
        "It provides tools for building and training neural networks.\n",
        "\n",
        "----------------\n",
        "##import torch.nn as nn:\n",
        "This imports the nn module from PyTorch, which contains building blocks for neural networks like layers, activation functions, and loss functions.\n",
        "\n",
        "----------------\n",
        "##import torchvision.transforms as transforms:\n",
        "This imports the transforms module from torchvision, a utility library for computer vision that works seamlessly with PyTorch.\n",
        "\n",
        "This module provides tools for transforming images, such as resizing, cropping, and converting them to tensors.\n",
        "\n",
        "----------------\n",
        "##import torchvision.datasets as datasets:\n",
        "\n",
        "This imports the datasets module from torchvision, which provides access to popular datasets like MNIST, CIFAR-10, and ImageNet.\n",
        "\n",
        "##CIFAR-10\n",
        "is a popular image dataset commonly used in computer vision and deep learning research for tasks like image classification.\n",
        "\n",
        "Here's a breakdown:\n",
        "\n",
        "###What it is:\n",
        "\n",
        "Collection of small images: It consists of 60,000 color images, each with a size of 32x32 pixels.\n",
        "\n",
        "###10 object classes:\n",
        "The images are divided into 10 different classes or categories, representing common objects like airplanes, automobiles, birds, cats, deer, dogs, frogs, horses, ships, and trucks.\n",
        "\n",
        "###Training and testing sets:\n",
        "\n",
        "The dataset is split into 50,000 training images and 10,000 testing images.\n",
        "\n",
        "###Why it's used:\n",
        "\n",
        "####Benchmark dataset:\n",
        "\n",
        "CIFAR-10 is often used as a benchmark dataset to evaluate the performance of image classification models.\n",
        "\n",
        "####Beginner-friendly:\n",
        "Due to its relatively small size and well-defined classes, it's a good starting point for beginners in deep learning.\n",
        "\n",
        "####Research and experimentation:\n",
        "\n",
        "Researchers use CIFAR-10 to explore different model architectures, training techniques, and data augmentation strategies.\n",
        "\n",
        "It simplifies the process of loading and using these datasets.\n",
        "\n",
        "--------------------\n",
        "##from torch.autograd import Variable:\n",
        "\n",
        "This imports the Variable class from PyTorch's autograd module.\n",
        "\n",
        "In older versions of PyTorch, this was used to wrap tensors for automatic differentiation.\n",
        "\n",
        "In newer versions, tensors are automatically tracked for gradients, so Variable might be less commonly used.\n",
        "\n",
        "-------------------\n",
        "##import matplotlib.pyplot as plt:\n",
        "This imports the pyplot module from Matplotlib as plt. Matplotlib is a widely used library for creating visualizations in Python.\n",
        "\n",
        "You'll likely use it to display images and plot graphs.\n",
        "\n",
        "-------------------\n",
        "##from PIL import Image:\n",
        "This imports the Image class from the Pillow (PIL) library.\n",
        "\n",
        "Pillow is a powerful library for image manipulation and processing.\n",
        "\n",
        "You might use it to open, resize, or convert images.\n",
        "\n",
        "-------------------------\n",
        "##import numpy as np:\n",
        "This imports the NumPy library as np. NumPy is a fundamental library for numerical computing in Python.\n",
        "\n",
        "It provides support for arrays, matrices, and mathematical functions. You might use it to work with image data represented as NumPy arrays.\n",
        "\n",
        "----------------------------\n",
        "#In summary\n",
        "These imports bring in the tools needed to load, process, and visualize images, build and train a neural network model, and perform deep learning operations.\n",
        "\n",
        "This setup is common for image classification, object detection, and other computer vision tasks using PyTorch."
      ],
      "metadata": {
        "id": "5SUpTEd_jIR8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oii0oVaegoXW"
      },
      "outputs": [],
      "source": [
        "# Specify the Mean and standard deviation of all the pixels in the MNIST dataset. They are precomputed\n",
        "mean_gray = 0.1307\n",
        "stddev_gray = 0.3081\n",
        "\n",
        "#Transform the images to tensors\n",
        "#Normalize a tensor image with mean and standard deviation. Given mean: (M1,...,Mn) and std: (S1,..,Sn)\n",
        "#for n channels, this transform will normalize each channel of the input torch.Tensor\n",
        "#i.e. input[channel] = (input[channel] - mean[channel]) / std[channel]\n",
        "\n",
        "transforms_ori = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((mean_gray,), (stddev_gray,))])\n",
        "\n",
        "transforms_photo = transforms.Compose([transforms.Resize((28,28)),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize((mean_gray,), (stddev_gray,))])\n",
        "\n",
        "#Load our dataset\n",
        "train_dataset = datasets.MNIST(root = './data',\n",
        "                            train = True,\n",
        "                            transform = transforms_ori,\n",
        "                            download = True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root = './data',\n",
        "                            train = False,\n",
        "                            transform = transforms_ori)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and Preprocessing the MNIST dataset\n",
        "This code is primarily focused on loading and preprocessing the MNIST dataset for use in a deep learning model, likely using PyTorch.\n",
        "\n",
        "-----------------\n",
        "##1. Specifying Mean and Standard Deviation:\n",
        "\n",
        "    mean_gray = 0.1307\n",
        "    stddev_gray = 0.3081\n",
        "\n",
        "These lines define the precomputed mean and standard deviation of the pixel values in the MNIST dataset.\n",
        "\n",
        "These values are essential for normalizing the image data, which is a common preprocessing step in deep learning.\n",
        "\n",
        "Normalization helps improve the training process and model performance.\n",
        "\n",
        "--------------------\n",
        "##2. Defining Transformations:\n",
        "\n",
        "    transforms_ori = transforms.Compose\n",
        "       ([transforms.ToTensor(),\n",
        "        transforms.Normalize((mean_gray,), (stddev_gray,))])\n",
        "\n",
        "    transforms_photo = transforms.Compose   \n",
        "      ([transforms.Resize((28,28)),\n",
        "       transforms.ToTensor(),\n",
        "       transforms.Normalize((mean_gray,), (stddev_gray,))])\n",
        "\n",
        "Here, two transformation pipelines are defined using torchvision.transforms.Compose.\n",
        "\n",
        "##transforms_ori:\n",
        "This transformation is intended for the original MNIST images.\n",
        "\n",
        "It first converts the image to a PyTorch tensor using transforms.ToTensor() and then normalizes the tensor using the precomputed mean and standard deviation using transforms.Normalize().\n",
        "\n",
        "=========================================\n",
        "##Note:\n",
        "The reason for the comma, after the mean and standard deviation, is that transforms.\n",
        "\n",
        "Normalize expects the mean and std arguments to be sequences (tuples or lists), even if you're only providing a single value for a grayscale image.\n",
        "\n",
        "###For grayscale images:\n",
        "MNIST images are grayscale, meaning they have only one channel.\n",
        "\n",
        "Therefore, you provide a single value for the mean and standard deviation.\n",
        "\n",
        "But, to make it a sequence, you need to add a comma after the value.\n",
        "\n",
        "This creates a tuple with one element.\n",
        "\n",
        "###For color images:\n",
        "If you were working with color images (like CIFAR-10), which have three channels (Red, Green, Blue), you would provide three values for the mean and standard deviation, like this:\n",
        "\n",
        "    transforms.Normalize((mean_R, mean_G, mean_B), (std_R, std_G, std_B))\n",
        "\n",
        "In your case, since you're dealing with grayscale MNIST images, the comma creates a single-element tuple for the mean and standard deviation, satisfying the expected input format for transforms.Normalize.\n",
        "\n",
        "----------------------------------\n",
        "##transforms_photo:\n",
        "This transformation is likely intended for external photos or images.\n",
        "\n",
        "It first resizes the image to 28x28 pixels using transforms.Resize((28,28)), converts it to a tensor, and then normalizes it similar to transforms_ori.\n",
        "\n",
        "-----------------------\n",
        "##3. Loading the Dataset:\n",
        "\n",
        "    train_dataset = datasets.MNIST(root = './data',\n",
        "       train = True,\n",
        "       transform = transforms_ori,\n",
        "       download = True)\n",
        "\n",
        "test_dataset = datasets.MNIST(root = './data',\n",
        "    train = False,\n",
        "    transform = transforms_ori)\n",
        "\n",
        "These lines load the MNIST dataset using torchvision.datasets.MNIST.\n",
        "\n",
        "----------------------\n",
        "##train_dataset:\n",
        "This loads the training portion of the MNIST dataset. root='./data' specifies where the data should be stored.\n",
        "\n",
        "    train=True indicates it's the training set.\n",
        "    \n",
        "    transform=transforms_ori\n",
        "    \n",
        "applies the defined transformation to the training images.\n",
        "\n",
        "    download=True\n",
        "\n",
        "downloads the dataset if it's not already present.\n",
        "\n",
        "------------------------------\n",
        "##test_dataset:\n",
        "This loads the testing portion of the MNIST dataset.\n",
        "\n",
        "It's similar to train_dataset but with train=False to indicate the testing set.\n",
        "\n",
        "In essence, this code prepares the MNIST dataset for deep learning by:\n",
        "\n",
        "Specifying the normalization parameters.\n",
        "\n",
        "Defining transformations to convert images to tensors and normalize them.\n",
        "Loading the training and testing datasets with the applied transformations.\n",
        "\n",
        "----------------------------------------\n",
        "This preprocessing is crucial for ensuring the data is in a suitable format for training a deep learning model effectively.\n",
        "\n",
        "Let me know if you have further questions about any specific part of the code or the MNIST dataset."
      ],
      "metadata": {
        "id": "JMEVH9gHmVjQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SSF2kD6goXW"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "random_image = train_dataset[20][0].numpy() * stddev_gray + mean_gray\n",
        "plt.imshow(random_image.reshape(28, 28), cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize a random image\n",
        " It's essentially visualizing a random image from the MNIST dataset that you loaded earlier. Here's a step-by-step explanation:\n",
        "\n",
        "--------------------------\n",
        "##import matplotlib.pyplot as plt:\n",
        "This line imports the pyplot module from the Matplotlib library, which is a widely used plotting library in Python.\n",
        "\n",
        "We use the alias plt for convenience.\n",
        "\n",
        "    random_image = train_dataset[20][0].numpy() * stddev_gray + mean_gray:\n",
        "    \n",
        "This line does the following:\n",
        "\n",
        "##train_dataset[20][0]\n",
        "accesses the 21st image (index 20) and its corresponding data (index 0) from the train_dataset.\n",
        "\n",
        "--------------------------------\n",
        "##.numpy()\n",
        "converts the image data from a PyTorch tensor to a NumPy array, which is a common format for working with images in Python.\n",
        "\n",
        "##* stddev_gray + mean_gray\n",
        "performs the inverse of the normalization that was applied during preprocessing.\n",
        "\n",
        "This is necessary to bring the pixel values back to the original scale for visualization.\n",
        "\n",
        "---------------------------\n",
        "##plt.imshow(random_image.reshape(28, 28), cmap='gray'):\n",
        "This line displays the image using Matplotlib's imshow function.\n",
        "\n",
        "-----------------------\n",
        "##random_image.reshape(28, 28)\n",
        "reshapes the image data into a 28x28 grid, representing the dimensions of the MNIST images.\n",
        "\n",
        "------------------------\n",
        "##cmap='gray'\n",
        "specifies that the image should be displayed in grayscale.\n",
        "\n",
        "To see the output, run the code.\n",
        "\n",
        "-------------------------\n",
        "#In essence\n",
        "This code snippet retrieves a random image from the MNIST training dataset, reverses the normalization, and then displays it as a grayscale image using Matplotlib.\n",
        "\n",
        "It's a common way to visually inspect the data you're working with in deep learning projects."
      ],
      "metadata": {
        "id": "n_xff3Sdyk9v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0kk1a02goXX"
      },
      "outputs": [],
      "source": [
        "print(train_dataset[20][1].item())   #Print the corresponding label for the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tejbqIdAgoXX"
      },
      "outputs": [],
      "source": [
        "batch_size = 100\n",
        "epochs = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elEWmxJMgoXX"
      },
      "outputs": [],
      "source": [
        "#Make the dataset iterable\n",
        "train_load = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                         batch_size = batch_size,\n",
        "                                         shuffle = True)\n",
        "\n",
        "test_load = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                         batch_size = batch_size,\n",
        "                                         shuffle = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make the dataset iterable\n",
        "\n",
        "The code snippet you provided is using PyTorch's DataLoader to make the training and testing datasets iterable, which is a crucial step for efficient training of deep learning models.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "---\n",
        "Here's a detailed explanation:\n",
        "\n",
        "##Purpose:\n",
        "\n",
        "The primary goal of this code is to create data loaders (train_load and test_load) that allow you to iterate through the training and testing datasets in batches during the training process.\n",
        "\n",
        "##This is essential for:\n",
        "\n",
        "##Batch Processing:\n",
        "\n",
        "Instead of feeding the entire dataset to the model at once, it's processed in smaller batches, which is more memory-efficient and allows for faster training.\n",
        "\n",
        "-------------------------\n",
        "##Shuffling:\n",
        "The shuffle=True argument for the training data loader randomizes the order of samples in each epoch.\n",
        "\n",
        "This helps the model generalize better and prevents it from learning patterns specific to the order of the data.\n",
        "\n",
        "-------------------------\n",
        "Explanation:\n",
        "\n",
        "    torch.utils.data.DataLoader:\n",
        "    \n",
        "This is the PyTorch class used to create data loaders.\n",
        "\n",
        "##dataset:\n",
        "This argument specifies the dataset you want to iterate over. In this case, it's train_dataset for training and test_dataset for testing, which you likely loaded earlier using torchvision.datasets.\n",
        "\n",
        "-------------------------\n",
        "##batch_size:\n",
        "This determines the number of samples in each batch.\n",
        "\n",
        "It's set to batch_size, which is a variable you probably defined earlier (e.g., batch_size = 100).\n",
        "\n",
        "##shuffle:\n",
        "This controls whether the data is shuffled before each epoch.\n",
        "\n",
        "It's set to True for the training data loader to randomize the samples and False for the testing data loader to keep the order consistent.\n",
        "\n",
        "--------------\n",
        "##How it Works:\n",
        "\n",
        "##Training:\n",
        "When you iterate through train_load, it will yield batches of data from train_dataset with the specified batch_size.\n",
        "\n",
        "The data will be shuffled before each epoch. This is commonly used in a training loop to feed data to the model.\n",
        "\n",
        "##Testing:\n",
        "When you iterate through test_load, it will yield batches of data from test_dataset with the specified batch_size.\n",
        "\n",
        "The data will not be shuffled, ensuring consistent evaluation. This is used after training to assess the model's performance on unseen data.\n",
        "\n",
        "In simpler terms:\n",
        "\n",
        "Imagine you have a deck of cards (your dataset).\n",
        "\n",
        "You want to deal the cards to players (your model) in groups (batches) for a game (training).\n",
        "\n",
        "DataLoader is like the dealer, organizing the cards.\n",
        "\n",
        "batch_size is the number of cards dealt to each player in a round.\n",
        "\n",
        "shuffle=True means the dealer shuffles the deck before each round of the game, making it more unpredictable.\n",
        "\n",
        "shuffle=False means the dealer keeps the cards in the original order, useful for comparing scores fairly.\n",
        "\n",
        "\n",
        "By using DataLoader, you efficiently manage the flow of data during training and evaluation, making the process smoother and more effective."
      ],
      "metadata": {
        "id": "PnMdoxZsYe2w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8AqeEsTgoXX"
      },
      "outputs": [],
      "source": [
        "print('There are {} images in the training set'.format(len(train_dataset)))\n",
        "print('There are {} images in the test set'.format(len(test_dataset)))\n",
        "print('There are {} batches in the train loader'.format(len(train_load)))\n",
        "print('There are {} batches in the testloader'.format(len(test_load)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-2r0-z1goXY"
      },
      "outputs": [],
      "source": [
        "#Create the model class\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN,self).__init__()\n",
        "        #Same Padding = [(filter size - 1) / 2] (Same Padding--> input size = output size)\n",
        "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3,stride=1, padding=1)\n",
        "        #The output size of each of the 8 feature maps is\n",
        "        #[(input_size - filter_size + 2(padding) / stride) +1] --> [(28-3+2(1)/1)+1] = 28 (padding type is same)\n",
        "        #Batch normalization\n",
        "        self.batchnorm1 = nn.BatchNorm2d(8)\n",
        "        #RELU\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "        #After max pooling, the output of each feature map is now 28/2 = 14\n",
        "        self.cnn2 = nn.Conv2d(in_channels=8, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
        "        #Output size of each of the 32 feature maps remains 14\n",
        "        self.batchnorm2 = nn.BatchNorm2d(32)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "        #After max pooling, the output of each feature map is 14/2 = 7\n",
        "        #Flatten the feature maps. You have 32 feature maps, each of them is of size 7x7 --> 32*7*7 = 1568\n",
        "        self.fc1 = nn.Linear(in_features=1568, out_features=600)\n",
        "        self.droput = nn.Dropout(p=0.5)\n",
        "        self.fc2 = nn.Linear(in_features=600, out_features=10)\n",
        "    def forward(self,x):\n",
        "        out = self.cnn1(x)\n",
        "        out = self.batchnorm1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.maxpool1(out)\n",
        "        out = self.cnn2(out)\n",
        "        out = self.batchnorm2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.maxpool2(out)\n",
        "        #Now we have to flatten the output. This is where we apply the feed forward neural network as learned before!\n",
        "        #It will take the shape (batch_size, 1568) = (100, 1568)\n",
        "        out = out.view(-1,1568)\n",
        "        #Then we forward through our fully connected layer\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.droput(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the model class\n",
        "This code defines a Convolutional Neural Network (CNN) class named CNN using PyTorch's nn.Module.\n",
        "\n",
        "This class is designed for image classification, specifically for the MNIST dataset which contains handwritten digits.\n",
        "\n",
        "----\n",
        "---\n",
        "\n",
        "\n",
        "Here's a step-by-step explanation:\n",
        "\n",
        "-------------------------\n",
        "#1. Class Definition:\n",
        "\n",
        "\n",
        "    class CNN(nn.Module):\n",
        "        def __init__(self):\n",
        "            super(CNN,self).__init__()\n",
        "            # ... (Layer definitions) ...\n",
        "\n",
        "         def forward(self,x):\n",
        "            # ... (Forward pass logic) ...\n",
        "\n",
        "\n",
        "##class CNN(nn.Module):\n",
        "\n",
        "This line defines a class named CNN that inherits from nn.Module, which is the base class for all neural network modules in PyTorch.\n",
        "\n",
        "##def __init__(self):\n",
        "This is the constructor of the class.\n",
        "\n",
        "It initializes the layers of the CNN.\n",
        "\n",
        "##super(CNN,self).__init__():\n",
        "This line calls the constructor of the parent class (nn.Module) to ensure proper initialization.\n",
        "\n",
        "##def forward(self,x):\n",
        "This defines the forward pass of the network, specifying how the input data (x) flows through the layers to produce the output.\n",
        "\n",
        "--------------------------------\n",
        "#2. Layer Definitions (within __init__)\n",
        "\n",
        "Convolutional Layers:\n",
        "\n",
        "    self.cnn1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3,stride=1, padding=1)\n",
        "\n",
        "    self.cnn2 = nn.Conv2d(in_channels=8, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
        "\n",
        "##These lines define two convolutional layers (cnn1 and cnn2).\n",
        "\n",
        "##in_channels:\n",
        "Number of input channels (1 for grayscale images in MNIST).\n",
        "\n",
        "##out_channels:\n",
        "Number of output channels (also called filters).\n",
        "\n",
        "cnn1 produces 8 feature maps, and cnn2 produces 32.\n",
        "\n",
        "##kernel_size:\n",
        "Size of the convolutional kernel (filter) - 3x3 for cnn1 and 5x5 for cnn2.\n",
        "\n",
        "##stride:\n",
        "The step size of the kernel as it moves across the input image.\n",
        "\n",
        "##padding:\n",
        "The amount of padding added to the input to control the output size. 'Same' padding ensures the output size is the same as the input size.\n",
        "\n",
        "-----------------------\n",
        "##Batch Normalization:\n",
        "\n",
        "    self.batchnorm1 = nn.BatchNorm2d(8)\n",
        "    self.batchnorm2 = nn.BatchNorm2d(32)\n",
        "\n",
        "These layers normalize the activations of the previous convolutional layers, which helps in training the network faster and more stable.\n",
        "\n",
        "The argument (8 or 32) specifies the number of features (channels) to normalize.\n",
        "\n",
        "-----------------------------------\n",
        "##Activation Function (ReLU):\n",
        "\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "This applies the Rectified Linear Unit (ReLU) activation function, which introduces non-linearity to the network.\n",
        "\n",
        "----------------------------------------\n",
        "##Max Pooling:\n",
        "\n",
        "    self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "    self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "These layers perform max pooling, which reduces the spatial dimensions of the feature maps, downsampling them by a factor of 2 in each dimension.\n",
        "\n",
        "---------------------------\n",
        "##Fully Connected Layers:\n",
        "\n",
        "    self.fc1 = nn.Linear(in_features=1568, out_features=600)\n",
        "\n",
        "    self.fc2 = nn.Linear(in_features=600, out_features=10)\n",
        "\n",
        "These are the fully connected layers that perform the final classification.\n",
        "\n",
        "-----------------------------------\n",
        "##fc1\n",
        "takes the flattened output of the convolutional layers (1568 features) and maps it to 600 features.\n",
        "\n",
        "##fc2\n",
        "maps the 600 features to 10 output features, representing the 10 digit classes (0-9).\n",
        "\n",
        "---------------------------------------\n",
        "##Dropout:\n",
        "\n",
        "    self.droput = nn.Dropout(p=0.5)\n",
        "\n",
        "This layer randomly sets a fraction (p=0.5) of the input units to 0 during training, which helps prevent overfitting.\n",
        "\n",
        "----------------------------------\n",
        "---------------------------\n",
        "#3. Forward Pass (within forward)\n",
        "\n",
        "This method defines how the input data flows through the layers:\n",
        "\n",
        "    out = self.cnn1(x)  # Convolution 1\n",
        "    out = self.batchnorm1(out)  # Batch Normalization 1\n",
        "    out = self.relu(out)  # ReLU activation\n",
        "    out = self.maxpool1(out)  # Max Pooling 1\n",
        "  # ... (Similar steps for cnn2, batchnorm2, relu, maxpool2) ...\n",
        "\n",
        "    out = out.view(-1,1568)  # Flatten the output\n",
        "    out = self.fc1(out)  # Fully connected 1\n",
        "    out = self.relu(out)  # ReLU activation\n",
        "    out = self.droput(out)  # Dropout\n",
        "    out = self.fc2(out)  # Fully connected 2 (output layer)\n",
        "  return out\n",
        "\n",
        "-------------------------\n",
        "-------------------------------------------\n",
        "----\n",
        "#In summary\n",
        "This code defines a CNN model with two convolutional layers, batch normalization, ReLU activation, max pooling, two fully connected layers, and dropout.\n",
        "\n",
        "The forward method specifies how data flows through these layers to produce the final output, which is a prediction for the digit class.\n",
        "\n"
      ],
      "metadata": {
        "id": "7UTXoX9nO7jR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VVxnyW_3goXY"
      },
      "outputs": [],
      "source": [
        "model = CNN()\n",
        "CUDA = torch.cuda.is_available()\n",
        "if CUDA:\n",
        "    model = model.cuda()\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVyG3TjPgoXY"
      },
      "outputs": [],
      "source": [
        "#Understand what's happening\n",
        "iteration = 0\n",
        "correct_nodata = 0\n",
        "correct_data = 0\n",
        "for i,(inputs,labels) in enumerate (train_load):\n",
        "    if iteration==1:\n",
        "        break\n",
        "    inputs = Variable(inputs)\n",
        "    labels = Variable(labels)\n",
        "    if torch.cuda.is_available():\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "    print(\"For one iteration, this is what happens:\")\n",
        "    print(\"Input Shape:\",inputs.shape)\n",
        "    print(\"Labels Shape:\",labels.shape)\n",
        "    output = model(inputs)\n",
        "    print(\"Outputs Shape\",output.shape)\n",
        "    _, predicted_nodata = torch.max(output, 1)\n",
        "    print(\"Predicted Shape\",predicted_nodata.shape)\n",
        "    print(\"Predicted Tensor:\")\n",
        "    print(predicted_nodata)\n",
        "    correct_nodata += (predicted_nodata == labels).sum()\n",
        "    print(\"Correct Predictions: \",correct_nodata)\n",
        "    _, predicted_data = torch.max(output.data, 1)\n",
        "    correct_data += (predicted_data == labels.data).sum()\n",
        "    print(\"Correct Predictions:\",correct_data)\n",
        "\n",
        "\n",
        "    iteration += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process of Iteration\n",
        "This code snippet is designed to help you understand the process of one iteration in the training loop of your CNN model on the MNIST dataset.\n",
        "\n",
        "---------------------------\n",
        "##Overall Goal:\n",
        "The code aims to demonstrate what happens during a single iteration of training, including:\n",
        "\n",
        "      1. Data Loading\n",
        "      2. Data Transfer to GPU (if available)\n",
        "      3. Forward Pass (Prediction)\n",
        "      4. Calculation of Correct Predictions\n",
        "      Initialization\n",
        "\n",
        "\n",
        "    iteration = 0\n",
        "    correct_nodata = 0\n",
        "    correct_data = 0\n",
        "\n",
        "iteration: A counter to track the number of iterations.\n",
        "\n",
        "correct_nodata: A variable to accumulate the number of correct predictions (without using .data).\n",
        "\n",
        "correct_data: A variable to accumulate the number of correct predictions (using .data).\n",
        "\n",
        "--------------------------\n",
        "#Iteration Loop\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(train_load):\n",
        "       if iteration == 1:\n",
        "         break\n",
        "\n",
        "This loop iterates through the train_load, which is a DataLoader you created earlier to load data in batches.\n",
        "\n",
        "enumerate provides both the index (i) and the data (inputs, labels) from the\n",
        "DataLoader.\n",
        "\n",
        "The loop is designed to execute only once (break when iteration is 1).\n",
        "\n",
        "-----------------------------\n",
        "#Data Preparation\n",
        "\n",
        "    inputs = Variable(inputs)\n",
        "       labels = Variable(labels)\n",
        "       if torch.cuda.is_available():\n",
        "         inputs = inputs.cuda()\n",
        "         labels = labels.cuda()\n",
        "\n",
        "##Variable(inputs) and Variable(labels):\n",
        "These lines wrap the input and label tensors in Variable objects.\n",
        "\n",
        "However, this might be outdated syntax as, in newer PyTorch versions, tensors are automatically tracked for gradients.\n",
        "\n",
        "##if torch.cuda.is_available():\n",
        "This checks if a CUDA-enabled GPU is available.\n",
        "\n",
        "##inputs = inputs.cuda() and labels = labels.cuda():\n",
        "If a GPU is available, the input and label tensors are moved to the GPU for faster processing.\n",
        "\n",
        "-------------------------\n",
        "##Forward Pass and Prediction\n",
        "\n",
        "    output = model(inputs)\n",
        "       _, predicted_nodata = torch.max(output, 1)\n",
        "       _, predicted_data = torch.max(output.data, 1)\n",
        "\n",
        "##output = model(inputs):\n",
        "This line performs a forward pass through your CNN model (model) with the input data (inputs).\n",
        "\n",
        "The output contains the model's predictions.\n",
        "    torch.max(output, 1) and torch.max(output.data, 1):\n",
        "\n",
        "These lines find the index of the maximum value along dimension 1 of the output tensor.\n",
        "\n",
        "This index represents the predicted class label. predicted_nodata uses the output tensor directly, while predicted_data uses the .data attribute (which might have subtle differences in gradient tracking).\n",
        "\n",
        "------------------------------\n",
        "##Calculating Correct Predictions\n",
        "\n",
        "    correct_nodata += (predicted_nodata ==\n",
        "    labels).sum()\n",
        "    correct_data += (predicted_data == labels.data).sum()\n",
        "\n",
        "These lines compare the predicted labels (predicted_nodata, predicted_data) with the actual labels (labels) and calculate the number of correct predictions.\n",
        "\n",
        "The results are accumulated in correct_nodata and correct_data, respectively.\n",
        "\n",
        "------------------------\n",
        "##Printing Information\n",
        "\n",
        "    print(\"For one iteration, this is what happens:\")\n",
        "      print(\"Input Shape:\", inputs.shape)\n",
        "      # ... (Other print statements) ...\n",
        "\n",
        "This section prints various information to the console, such as the shapes of input, labels, outputs, predicted labels, and the number of correct predictions.\n",
        "\n",
        "This helps in understanding the flow of data and the results of the iteration.\n",
        "\n",
        "---------------------\n",
        "#Iteration Update\n",
        "    iteration += 1\n",
        "\n",
        "This line increments the iteration counter by 1, moving to the next iteration (if there were any).\n",
        "\n",
        "However, since the loop breaks after one iteration, this line is not executed in this snippet.\n",
        "\n",
        "I hope this explanation helps you understand what's happening in the code."
      ],
      "metadata": {
        "id": "Qnc24Ghc8GZW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jGpYJMcH75_7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30OuNqWcgoXZ"
      },
      "outputs": [],
      "source": [
        "#Training the CNN\n",
        "num_epochs = 2\n",
        "\n",
        "#Define the lists to store the results of loss and accuracy\n",
        "train_loss = []\n",
        "test_loss = []\n",
        "train_accuracy = []\n",
        "test_accuracy = []\n",
        "\n",
        "#Training\n",
        "for epoch in range(num_epochs):\n",
        "    #Reset these below variables to 0 at the begining of every epoch\n",
        "    correct = 0\n",
        "    iterations = 0\n",
        "    iter_loss = 0.0\n",
        "\n",
        "    model.train()                   # Put the network into training mode\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(train_load):\n",
        "\n",
        "        # Convert torch tensor to Variable\n",
        "        inputs = Variable(inputs)\n",
        "        labels = Variable(labels)\n",
        "\n",
        "        # If we have GPU, shift the data to GPU\n",
        "        CUDA = torch.cuda.is_available()\n",
        "        if CUDA:\n",
        "            inputs = inputs.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "        optimizer.zero_grad()            # Clear off the gradient in (w = w - gradient)\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        iter_loss += loss.data[0]       # Accumulate the loss\n",
        "        loss.backward()                 # Backpropagation\n",
        "        optimizer.step()                # Update the weights\n",
        "\n",
        "        # Record the correct predictions for training data\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum()\n",
        "        iterations += 1\n",
        "\n",
        "    # Record the training loss\n",
        "    train_loss.append(iter_loss/iterations)\n",
        "    # Record the training accuracy\n",
        "    train_accuracy.append((100 * correct / len(train_dataset)))\n",
        "\n",
        "    #Testing\n",
        "    loss = 0.0\n",
        "    correct = 0\n",
        "    iterations = 0\n",
        "\n",
        "    model.eval()                    # Put the network into evaluation mode\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(test_load):\n",
        "\n",
        "        # Convert torch tensor to Variable\n",
        "        inputs = Variable(inputs)\n",
        "        labels = Variable(labels)\n",
        "\n",
        "        CUDA = torch.cuda.is_available()\n",
        "        if CUDA:\n",
        "            inputs = inputs.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, labels) # Calculate the loss\n",
        "        loss += loss.data[0]\n",
        "        # Record the correct predictions for training data\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum()\n",
        "\n",
        "        iterations += 1\n",
        "\n",
        "    # Record the Testing loss\n",
        "    test_loss.append(loss/iterations)\n",
        "    # Record the Testing accuracy\n",
        "    test_accuracy.append((100 * correct / len(test_dataset)))\n",
        "\n",
        "    print ('Epoch {}/{}, Training Loss: {:.3f}, Training Accuracy: {:.3f}, Testing Loss: {:.3f}, Testing Acc: {:.3f}'\n",
        "           .format(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1],\n",
        "             test_loss[-1], test_accuracy[-1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xTtmkMn3goXZ"
      },
      "outputs": [],
      "source": [
        "#Run this if you want to save the model\n",
        "torch.save(model.state_dict(),'CNN_MNIST.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoktYonfgoXZ"
      },
      "outputs": [],
      "source": [
        "# Loss\n",
        "f = plt.figure(figsize=(10, 10))\n",
        "plt.plot(train_loss, label='Training Loss')\n",
        "plt.plot(test_loss, label='Testing Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lnbes_CfgoXZ"
      },
      "outputs": [],
      "source": [
        "# Accuracy\n",
        "f = plt.figure(figsize=(10, 10))\n",
        "plt.plot(train_accuracy, label='Training Accuracy')\n",
        "plt.plot(test_accuracy, label='Testing Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HpYATwTgoXZ"
      },
      "outputs": [],
      "source": [
        "#Run this if you want to load the model\n",
        "model.load_state_dict(torch.load('CNN_MNIST.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tNhVHASSgoXZ"
      },
      "outputs": [],
      "source": [
        "#Predict your own image\n",
        "def predict(img_name,model):\n",
        "    image = cv2.imread(img_name,0)   #Read the image\n",
        "    ret, thresholded = cv2.threshold(image,127,255,cv2.THRESH_BINARY)   #Threshold the image\n",
        "    img = 255-thresholded           #Apply image negative\n",
        "    cv2.imshow('Original',img)      #Display the processed image\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "    img = Image.fromarray(img)      #Convert the image to an array\n",
        "    img = transforms_photo(img)     #Apply the transformations\n",
        "    img = img.view(1,1,28,28)       #Add batch size\n",
        "    img = Variable(img)             #Wrap the tensor to a variable\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda()\n",
        "        img = img.cuda()\n",
        "\n",
        "    output = model(img)\n",
        "    print(output)\n",
        "    print(output.data)\n",
        "    _, predicted = torch.max(output,1)\n",
        "    return  predicted.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3j4_Qug0goXa"
      },
      "outputs": [],
      "source": [
        "pred = predict('3.jpg', model)\n",
        "print(\"The Predicted Label is {}\".format(pred))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}